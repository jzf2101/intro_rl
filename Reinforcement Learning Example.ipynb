{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decison Processes\n",
    "\n",
    "In Markov Decision Processes, one's actions affect the state of the world.  \n",
    "\n",
    "Given a state $s$ and an action $a$, we know the probability it will end up in the next state $s'$ \n",
    "\n",
    "## Value Iteration/Backwards Induction/Dynamic Programming\n",
    "\n",
    "Value iteration estimates the value of the best action at each state, $V^*(s)$, with recursion since\n",
    "\n",
    "$$V^*(s) = \\max_a\\sum_{s'}P(s'|s,a)\\left(R(s,a,s') + \\gamma V^*(s')\\right)$$\n",
    "\n",
    "This equation is also known as the Bellman equation\n",
    "\n",
    "Because of the discount factor, $\\gamma$, our estimates will converge with the number of iterations since \n",
    "\n",
    "$$\\mid V^*_i(s) - V^*_{i+1}(s) \\mid  \\leq \\gamma^i\\mid R_{max} - R_{min}\\mid$$ \n",
    "\n",
    "\n",
    "## Gridworld Toy Example\n",
    "\n",
    "Gridworld, a typical Reinforcement Learning problem, requires the agent to navigate to a goal loation and avoid possible pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4faef97020b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinearmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/seaborn/linearmodels.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0m_has_statsmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/statsmodels/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimplefilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from .tools.sm_exceptions import (ConvergenceWarning, CacheWriteWarning,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                   IterationLimitWarning, InvalidTestWarning)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/statsmodels/tools/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/statsmodels/tools/tools.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m from statsmodels.distributions import (ECDF, monotone_fn_inverter,\n\u001b[1;32m     10\u001b[0m                                        StepFunction)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebuse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_matrix_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/site-packages/statsmodels/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#__all__ = filter(lambda s:not s.startswith('_'),dir())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from . import (anes96, cancer, committee, ccard, copper, cpunish, elnino,\n\u001b[0m\u001b[1;32m      6\u001b[0m                \u001b[0mengel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrunfeld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlongley\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacrodata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodechoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandhie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0mscotland\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstar98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrikes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msunspots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/Users/4d/anaconda/lib/python3.5/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "\n",
    "sns.set_context('poster')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's imagine a 3 by 4 grid with 1 goal and 1 pitfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = np.array([[-.01,-.01,-.01,1],[-.01,np.nan, -.01, -1],[-.01,-.01,-.01,-.01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5ad4f43115f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RdBu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot_kws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward for Each Grid Square'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(grid, cmap='RdBu', annot=True, cbar = False, annot_kws={\"size\": 14})\n",
    "plt.title('Reward for Each Grid Square')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this world the agent can go up, down, left and right\n",
    "\n",
    "### We'll assume 80% of the time the agent ends up going where it wants to go\n",
    "\n",
    "### 20% of the time it veers off perpendicularly\n",
    "\n",
    "### If it ends up in the pitfall or the goal, it's in the `exit` state and the game is over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']\n",
    "transition = pd.DataFrame(np.array([[.8, 0, .1, .1,], [0, .8, .1, .1], [.1, .1, .8, 0],\n",
    "                                    [.1, .1, 0, .8]]), index = actions, columns = actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(transition, annot=True, cbar = False, annot_kws={\"size\": 14})\n",
    "plt.title('Transition Probabilities for Each Action')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our policy is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = pd.DataFrame([['unknown', 'unknown', 'unknown', 'exit'],\n",
    "                       ['unknown', '', 'unknown', 'exit'],\n",
    "                       ['unknown', 'unknown', 'unknown', 'unknown']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll now use Value Iteration to Solve for the Optimal Policy\n",
    "\n",
    "Assume the discount factor is $0.75$\n",
    "\n",
    "Let's set up our function to update the Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.75\n",
    "row, col = np.shape(grid)\n",
    "nactions = len(actions)\n",
    "\n",
    "q_values = np.zeros((row,col,nactions))\n",
    "v_values = np.copy(grid)\n",
    "\n",
    "def index_grid(r, c, action):\n",
    "    '''\n",
    "    finds the right index for the next state based on the action name\n",
    "    if the agent tries to go in the direction of a wall or edge,\n",
    "    the agent says in the same place\n",
    "    \n",
    "    Args:\n",
    "        r: row\n",
    "        c: column\n",
    "        action: action name\n",
    "    \n",
    "    Returns:\n",
    "        r: updated row\n",
    "        c: updated column\n",
    "    '''\n",
    "    if  action == 'up':\n",
    "        if r > 0:\n",
    "            r -= 1\n",
    "    if action == 'down':\n",
    "        if r < row - 1:\n",
    "            r += 1\n",
    "    if action == 'left':\n",
    "        if c > 0:\n",
    "            c -= 1\n",
    "    if action == 'right':\n",
    "        if c < col - 1:\n",
    "            c += 1\n",
    "    return r, c\n",
    "\n",
    "def update_q(qvalues, vvalues):\n",
    "    '''\n",
    "    updates the value of each action given the current state and possible future states\n",
    "    recall that the value of each action is the expected value of the current action \n",
    "    + future scenarios (see bellman equation above)\n",
    "    \n",
    "    Args:\n",
    "        qvalues: array of qvalues for each state, action pair\n",
    "        vvalues: array of vvalues for each state\n",
    "    \n",
    "    Returns:\n",
    "        qvalues: updated qvalues\n",
    "    '''\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            #if we are in the exit squares, there is no where else to go so the value is fixed\n",
    "            if abs(v_values[r, c]) == 1:\n",
    "                qvalues[r, c, :] = v_values[r, c]\n",
    "                continue\n",
    "            #if we're in a nan square we can assume there's no value for that state\n",
    "            elif np.isnan(grid[r, c]):\n",
    "                qvalues[r, c, :] = np.nan\n",
    "            for i, s in enumerate(actions):\n",
    "                #we're going to loop over the actions twice because we're going from one state to another\n",
    "                rewards = np.zeros(len(actions))\n",
    "                for s_prime in range(len(rewards)):\n",
    "                    #transitions with 0 probabiliy we can ignore\n",
    "                    if transition[s].ix[actions[s_prime]] == 0:\n",
    "                        continue\n",
    "                    nrow, ncol = index_grid(r,c,s)\n",
    "                    #attempts to go to the empty square result on the agent staying put\n",
    "                    if np.isnan(grid[nrow, ncol]):\n",
    "                        rewards[s_prime] = grid[r, c] + gamma*vvalues[r, c]\n",
    "                    else:\n",
    "                        rewards[s_prime] = grid[r, c] + gamma*vvalues[nrow, ncol]\n",
    "                expected_value = np.dot(rewards, np.squeeze(transition[s]))\n",
    "                qvalues[r,c,i] = expected_value\n",
    "    return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's update the Q-values based on what we see on the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qvalues_1 = update_q(q_values, v_values)\n",
    "qvalues_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we know these Q-values we can update the V-values\n",
    "\n",
    "Recall that \n",
    "\n",
    "$$V^*(s) = \\max(Q^*(s,a))$$\n",
    "\n",
    "and the optimal policy, $\\pi^*(s)$, is\n",
    "\n",
    "$$\\pi^*(s) = \\text{argmax}(Q^*(s,a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_v(qvalues, policy):\n",
    "    '''\n",
    "    updates the policy based on the qvalues\n",
    "    recall that v*(s) = max(q*(s,a))\n",
    "    also pi*(s) = argmax(q*(s,a))\n",
    "    \n",
    "    Args:\n",
    "        qvalues: array of values for each state, action pair\n",
    "        policy: array of actions for each state\n",
    "    \n",
    "    Returns:\n",
    "        v_values_new: new array vvalues for each state\n",
    "        policy: updated policy array\n",
    "    '''\n",
    "    v_values_new = np.max(qvalues, axis = 2)\n",
    "    policy_indices = np.argmax(qvalues, axis = 2)\n",
    "    for r in policy.index:\n",
    "        for c in policy.columns:\n",
    "            #we can ignore the blank states and terminal states\n",
    "            if policy[c].ix[r] == 'exit' or policy[c].ix[r] == '':\n",
    "                continue\n",
    "            policy[c].ix[r] = actions[policy_indices[r, c]]\n",
    "    return v_values_new, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvalues_1, policy_1 = update_v(qvalues_1, policy)\n",
    "sns.heatmap(vvalues_1, cmap='RdBu', annot=True, cbar = False, annot_kws={\"size\": 14})\n",
    "plt.title('V(s) After 1 Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's run this one more time to see how these values change again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qvalues_2 = update_q(qvalues_1, vvalues_1)\n",
    "qvalues_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vvalues_2, policy_2 = update_v(qvalues_2, policy_1)\n",
    "sns.heatmap(vvalues_2, cmap='RdBu', annot=True, cbar = False, annot_kws={\"size\": 14})\n",
    "plt.title('V(s) after 2 Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now combine these two steps and run until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def value_iteration(qvalues, vvalues, policy, epsilon):\n",
    "    qvalues = update_q(qvalues, vvalues)\n",
    "    v_new, policy_new = update_v(qvalues, policy)  \n",
    "    print(abs(v_new - vvalues))\n",
    "    while (np.nanmax(abs(v_new - vvalues)) > epsilon):\n",
    "        print('current epsilon: %0.5f' % np.nanmax(abs(v_new - vvalues)))\n",
    "        vvalues = v_new\n",
    "        qvalues = update_q(qvalues, vvalues)\n",
    "        v_new, policy_new = update_v(qvalues, policy)\n",
    "        \n",
    "    return v_new, policy_new, qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_final, policy_final, qvalues = value_iteration(qvalues_2, vvalues_2, policy_2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(v_final, cmap='RdBu', annot=True, cbar = False, annot_kws={\"size\": 14})\n",
    "plt.title('Converged Values of V(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qvalues"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
